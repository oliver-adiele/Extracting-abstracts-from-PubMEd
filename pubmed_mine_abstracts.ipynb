{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "pubmed_mine_abstracts.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrCmfc5BaXD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs \n",
        "import requests \n",
        "from bs4 import BeautifulSoup as bs \n",
        "from slugify import slugify \n",
        "import pickle \n",
        "import time \n",
        "import os "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVwzojDTaXER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SearchTemplate = 'https://pubmed.ncbi.nlm.nih.gov/?term=Sars-Cov-2%20or%20COVID-19%20or%20nCOV-2019&size=100&page={}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fge8R_i4aXEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and cache a webpage if it hasn't been downloaded and cached before\n",
        "# NOTE: Requires the existence of a folder named 'searchBioPages'!\n",
        "def cacheQuery(query, forceUncache=False):\n",
        "    queryFile = 'searchBioPages/' + slugify(query)\n",
        "    if ((not forceUncache) and os.path.isfile(queryFile)):\n",
        "        data = pickle.load(open(queryFile, 'rb'))\n",
        "    else:\n",
        "        time.sleep(1) # Waiting 1 second is the minimum level of \"politeness\"\n",
        "        r = requests.get(query)\n",
        "        if (r.status_code == 200):\n",
        "            data = r.text\n",
        "            pickle.dump(data, open(queryFile, 'wb'))  \n",
        "        else:\n",
        "            data = None\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJSbF8HVaXE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse each search page for article listings\n",
        "def processSearchPage(page, query):\n",
        "\n",
        "    articleData = [] # The article listings for this page, indexed by DOI\n",
        "\n",
        "    html = bs(page, \"lxml\") # Initialize BeautifulSoup parser with lxml parsing module\n",
        "    \n",
        "  \n",
        "    article_lists = html.find('div', attrs={'class':'search-results-chunk results-chunk'})\n",
        "    article_markers = str(article_lists)[64:963]\n",
        "    articles = article_markers.replace(',','')\n",
        "    articles = articles.split()\n",
        "    articles = str(articles)[2:802]\n",
        "    i = 0\n",
        "    while i <len(str(articles)):\n",
        "        articleData.append(articles[i:i+8])\n",
        "        i = i+8\n",
        "    return articleData "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxUuWe7saXFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call the functions below to open a search page and copy the urls for the articles therein\n",
        "def getUrlsInRange(page_num):\n",
        "\n",
        "    global Create_file\n",
        "    \n",
        "    queryString = SearchTemplate.format(page_num)\n",
        "    page = cacheQuery(queryString)\n",
        "\n",
        "    if (page is not None):\n",
        "        ArticlePointers = processSearchPage(page, queryString)\n",
        "    \n",
        "    # Write all urls into a fileall the DOI info to a file\n",
        "        i = 0\n",
        "        while i < len(ArticlePointers):\n",
        "            Create_file.write(ArticlePointers[i] + '\\n')\n",
        "            i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks7UNG91aXFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the Urls for the articles in the search\n",
        "\n",
        "Create_file = codecs.open(\"links_comprehensive_pubmed_covid-19.txt\", 'w', 'utf-8')\n",
        "FirstPage = 1\n",
        "LastPage = 196\n",
        "\n",
        "# Set the start and end of the search pages\n",
        "d = FirstPage\n",
        "delta = 1\n",
        "\n",
        "# Step through the full page range, incrementing the start and end\n",
        "\n",
        "while d <= LastPage:\n",
        "    searchPage = d\n",
        "    getUrlsInRange(searchPage)\n",
        "    d += delta\n",
        "\n",
        "Create_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQcsx9pvaXGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prep links for extraction of data\n",
        "urls =[]\n",
        "with open(\"links_comprehensive_pubmed_covid-19.txt\",\"r\") as links:\n",
        "    for line in links:\n",
        "        urls.append(str(line)[:8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctCmFL08aXII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract abstracts from links\n",
        "\n",
        "abstractData = {}\n",
        "abstract = codecs.open(\"abstracts_compre_pubmed_covid-19.txt\",\"w\",encoding = \"utf-8\") \n",
        "item = 0\n",
        "while item < len(urls):\n",
        "    page_file = requests.get('https://pubmed.ncbi.nlm.nih.gov/'+str(urls[item]))\n",
        "    if (page_file.status_code == 200):\n",
        "        page_file = page_file.text\n",
        "        data = bs(page_file,'lxml')\n",
        "        abstract_file = data.find('div', attrs={'class':'abstract-content selected'})\n",
        "        if abstract_file is not None:\n",
        "            if abstract_file.p is not None:\n",
        "                if abstract_file.p.text is not None:\n",
        "                    abstractData[urls[item]] = abstract_file.p.text.strip()\n",
        "                    abstract.write(str(abstract_file.p.text.strip()) +\"\\n\")  \n",
        "    item = item+1\n",
        "abstract.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
